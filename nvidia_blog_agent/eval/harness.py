"""Evaluation harness for QA agent testing and regression.

This module provides:
- EvalCase: Test case definition with question and expected substrings
- EvalResult: Result of evaluating a single case
- EvalSummary: Summary statistics for a set of evaluation results
- simple_pass_fail_checker: Simple substring-based pass/fail checker
- run_qa_evaluation: Run evaluation over multiple cases using QAAgent
- summarize_eval_results: Compute summary statistics from results

The harness is designed to work with stubbed QAAgent implementations for
deterministic, fast testing without real LLM or RAG backend calls.
"""

from dataclasses import dataclass
from typing import List, Sequence
from nvidia_blog_agent.contracts.blog_models import RetrievedDoc
from nvidia_blog_agent.agents.qa_agent import QAAgent


@dataclass
class EvalCase:
    """Test case for QA evaluation.
    
    Attributes:
        question: The question to ask the QA agent.
        expected_substrings: List of substrings that must appear in the answer
                           (case-insensitive) for the case to pass.
        max_docs: Maximum number of documents to retrieve. Defaults to 5.
    """
    question: str
    expected_substrings: List[str]
    max_docs: int = 5


@dataclass
class EvalResult:
    """Result of evaluating a single EvalCase.
    
    Attributes:
        question: The question that was evaluated.
        answer: The answer generated by the QA agent.
        retrieved_docs: List of RetrievedDoc objects used to generate the answer.
        passed: True if all expected_substrings were found in the answer.
        matched_substrings: List of expected_substrings that were found in the answer.
    """
    question: str
    answer: str
    retrieved_docs: List[RetrievedDoc]
    passed: bool
    matched_substrings: List[str]


@dataclass
class EvalSummary:
    """Summary statistics for a set of evaluation results.
    
    Attributes:
        total: Total number of cases evaluated.
        passed: Number of cases that passed.
        failed: Number of cases that failed.
        pass_rate: Pass rate as a float between 0.0 and 1.0.
    """
    total: int
    passed: int
    failed: int
    pass_rate: float


def simple_pass_fail_checker(
    answer: str,
    expected_substrings: List[str]
) -> tuple[bool, List[str]]:
    """Check which expected_substrings appear in the answer (case-insensitive).
    
    This function performs a simple substring matching check. All expected
    substrings must appear in the answer (case-insensitive) for the check to pass.
    
    Args:
        answer: The answer string to check.
        expected_substrings: List of substrings that must appear in the answer.
    
    Returns:
        Tuple of (passed, matched_substrings), where:
        - passed: True if all expected_substrings were found, False otherwise
        - matched_substrings: List of expected_substrings that were found in the answer
    
    Example:
        >>> passed, matched = simple_pass_fail_checker(
        ...     "This is about NVIDIA RAG technology",
        ...     ["NVIDIA", "RAG"]
        ... )
        >>> passed
        True
        >>> len(matched) == 2
        True
    """
    if not expected_substrings:
        return True, []
    
    lower_answer = answer.lower()
    matched = [s for s in expected_substrings if s.lower() in lower_answer]
    passed = len(matched) == len(expected_substrings)
    return passed, matched


async def run_qa_evaluation(
    qa_agent: QAAgent,
    cases: Sequence[EvalCase],
) -> List[EvalResult]:
    """Run QA evaluation for the given cases using the provided QAAgent.
    
    This function evaluates each case by:
    1. Calling qa_agent.answer() with the case's question and max_docs
    2. Checking the answer against expected_substrings using simple_pass_fail_checker
    3. Creating an EvalResult for each case
    
    The qa_agent is assumed to be backed by a stub RAG client and stub model
    for testing purposes, making this evaluation deterministic and fast.
    
    Args:
        qa_agent: QAAgent instance to evaluate (typically with stubbed dependencies).
        cases: Sequence of EvalCase objects to evaluate.
    
    Returns:
        List of EvalResult objects, one per input case, in the same order.
    
    Example:
        >>> from nvidia_blog_agent.agents.qa_agent import QAAgent
        >>> agent = QAAgent(rag_client=stub_client, model=stub_model)
        >>> cases = [
        ...     EvalCase(question="What is RAG?", expected_substrings=["RAG"])
        ... ]
        >>> results = await run_qa_evaluation(agent, cases)
        >>> len(results) == 1
        True
    """
    results: List[EvalResult] = []
    
    for case in cases:
        answer, docs = await qa_agent.answer(case.question, k=case.max_docs)
        passed, matched = simple_pass_fail_checker(answer, case.expected_substrings)
        
        results.append(
            EvalResult(
                question=case.question,
                answer=answer,
                retrieved_docs=docs,
                passed=passed,
                matched_substrings=matched,
            )
        )
    
    return results


def summarize_eval_results(results: List[EvalResult]) -> EvalSummary:
    """Summarize EvalResult list into basic metrics.
    
    This function computes summary statistics from a list of evaluation results,
    including total count, pass/fail counts, and pass rate.
    
    Args:
        results: List of EvalResult objects to summarize.
    
    Returns:
        EvalSummary object with total, passed, failed, and pass_rate fields.
        Pass rate is 0.0 if results list is empty.
    
    Example:
        >>> results = [
        ...     EvalResult(question="Q1", answer="A1", retrieved_docs=[], passed=True, matched_substrings=["test"]),
        ...     EvalResult(question="Q2", answer="A2", retrieved_docs=[], passed=False, matched_substrings=[]),
        ... ]
        >>> summary = summarize_eval_results(results)
        >>> summary.total == 2
        True
        >>> summary.passed == 1
        True
        >>> summary.pass_rate == 0.5
        True
    """
    total = len(results)
    passed = sum(1 for r in results if r.passed)
    failed = total - passed
    pass_rate = float(passed) / total if total > 0 else 0.0
    
    return EvalSummary(
        total=total,
        passed=passed,
        failed=failed,
        pass_rate=pass_rate,
    )

